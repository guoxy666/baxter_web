# Abstract

zero one lab NB


# Introduction
   &nbsp;&nbsp;The ambition for robots to seamlessly integrate into human environments as capable assistants hinges on their ability to perform dexterous manipulation tasks. A fundamental aspect of this is task-oriented grasping: the robot must not merely seize an object, but grasp it in a manner conducive to a specific, often implicitly stated, goal [1], [2]. Consider tasks like preparing a meal or tidying a workshop; a robot must grasp a knife by its handle to cut vegetables safely, lift a screwdriver by its handle to tighten a screw effectively, or carefully grip a delicate wine glass by its stem to avoid spillage. These scenarios underscore the critical need for grasping strategies that are acutely aware of object affordances – the functional parts dictating appropriate interaction – and adhere to the constraints imposed by the task, object geometry, and intended use. This challenge is significantly amplified in open-vocabulary settings, where instructions may range from explicit commands ("pass me the handle of the blue screwdriver") to ambiguous requests ("I need to fasten this," "I'm thirsty"), requiring the robot to reason about intent, identify relevant objects amidst clutter, and infer the correct functional part for grasping, often for objects unseen during training [3], [4]. Furthermore, the physical reality of many objects – being too large, heavy, or unwieldy for a single gripper – necessitates coordinated dual-arm manipulation, adding layers of complexity related to grasp point selection, inter-arm collision avoidance, and maintaining stable control throughout the manipulation process [5].


  &nbsp;&nbsp;There have been many excellent recent approaches that address parts of this complex problem[6-9], but often fail to provide a unified solution.
Traditional grasp synthesis algorithms typically generate grasps uniformly across an object, lacking task-specificity and struggling with constraints [1, 2]. Recent works leverage Vision-Language Models (VLMs) or Large Language Models (LLMs) for improved reasoning. For instance, AffordGrasp [10] excels at interpreting implicit user instructions and grounding task-relevant affordances onto object parts using VLMs. ThinkGrasp [11] focuses on strategic reasoning to handle heavy clutter by identifying and removing obstructing objects. However, these reasoning-focused methods often rely on general-purpose grasp generators (like AnyGrasp [12] or GraspNet [13]) downstream, which may not be optimized for efficiently generating high-quality grasps precisely within the identified (potentially small or complex) constraint region. Other approaches focus specifically on constrained grasp generation, like VCGS [14], but typically require large datasets annotated with constraints for training. The CGDF method [15] introduced a powerful Part-Guided Diffusion strategy capable of generating sample-efficient constrained grasps on complex shapes without retraining, by conditioning on both global object geometry and the local constraint region during inference. However, CGDF itself lacks the upstream perception and reasoning capabilities to interpret tasks and define these constraint regions from open-vocabulary instructions. Furthermore, robust, task-oriented dual-arm grasping guided by language remains relatively underexplored, with existing methods often relying purely on geometric sampling [5] rather than semantic task understanding.


  &nbsp;&nbsp;To overcome these limitations, we introduce UniDiffGrasp, a unified framework engineered for open-vocabulary, task-oriented constrained grasping, proficient in both single-arm precision and cooperative dual-arm manipulation. UniDiffGrasp uniquely integrates the inferential power of VLMs for semantic understanding with the advanced capabilities of Constrained Grasp Diffusion Fields (CGDF) [15] for geometrically precise and efficient grasp generation. Our pipeline initiates with a VLM (e.g., GPT-4o) analyzing multimodal input (natural language instructions, visual scene) to deduce the task, identify the target object, and pinpoint the functionally relevant part or affordance region. This semantic target is then accurately localized using cutting-edge open-vocabulary segmentation (e.g., VLPart, groundedSAM) [16], [17], yielding a precise geometric definition (e.g., a partial point cloud) of the intended interaction zone.


  &nbsp;&nbsp;The core innovation lies in leveraging the Part-Guided Diffusion strategy native to CGDF. This allows UniDiffGrasp to utilize a diffusion model, pre-trained without any explicit constraint data, to generate a dense distribution of high-quality 6-DoF grasp poses focused specifically within the VLM-identified target region during inference. It achieves this remarkable capability by simultaneously evaluating grasp candidates against both the local geometry of the target part and the global context of the entire object, guiding the diffusion process towards poses that satisfy both local interaction requirements and global stability/collision constraints. This eliminates the need for constraint-specific retraining, achieves high sample efficiency, and robustly handles complex object geometries thanks to CGDF's effective shape representation. For dual-arm scenarios, UniDiffGrasp extends this strategy by defining two distinct target regions – potentially identified through VLM reasoning (e.g., "grasp both handles") or geometric sampling (e.g., farthest points on a large box) – and applying the Part-Guided Diffusion process independently for each arm. A subsequent rigorous selection module then evaluates candidate pairs based on individual grasp quality, mutual collision checks, and dual-arm stability metrics (e.g., force closure) to determine the optimal cooperative grasp.
Our contributions are as follows:


  &nbsp;&nbsp;We present an end-to-end framework that seamlessly connects advanced VLM-driven task understanding and affordance foundations with state-of-the-art diffusion-based constrained grasp generation. The framework is capable of interpreting implicit human commands and performing precise grasps in complex environments targeting specified object portions for both one-arm and coordinated two-arm tasks.


  &nbsp;&nbsp;We demonstrate the effective application of the Part-Guided Diffusion strategy guided directly by VLM-inferred semantic constraints, enabling zero-shot, task-oriented grasping on targeted regions of novel objects without constraint-specific retraining.


  &nbsp;&nbsp;We introduce a principled methodology for coordinated dual-arm affordance grasping, incorporating flexible target region definition, independent constrained grasp generation per arm via Part-Guided Diffusion, and robust pair selection for kinematic feasibility and stability.


  &nbsp;&nbsp;We validate UniDiffGrasp through extensive experiments, showcasing state-of-the-art performance in grasp success, sample efficiency, and robustness, particularly in challenging scenarios involving complex geometries, constraints, and dual-arm coordination on a physical robot platform.
